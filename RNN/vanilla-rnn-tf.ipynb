{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN on Tensorflow\n",
    "\n",
    "In this notebook, we'll build a character-wise rnn trained on Anna Karenina books.\n",
    "This tutorial is based on Andrej Karpathy's [RNN Post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning/raw/d94980095d1187998e2e0544966bb417f031390f/intro-to-rnns/assets/charseq.jpeg\" alt=\"Drawing\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f: # the dataset that we use\n",
    "    text=f.read()\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x\\n', array([[32, 65, 58, 73, 77, 62, 75,  2, 17,  1],\n",
      "       [ 2, 76, 69, 62, 62, 79, 62,  2, 72, 63],\n",
      "       [62, 75, 14,  4,  1,  0,  1,  0, 48, 77],\n",
      "       [76,  2, 70, 62, 58, 71, 66, 71, 64,  1],\n",
      "       [65, 62, 75, 12,  2, 35, 66, 71, 72, 64],\n",
      "       [61,  1,  0, 76, 72,  2, 65, 62,  2, 65],\n",
      "       [66, 60, 66, 71, 64,  2, 65, 62, 75,  2],\n",
      "       [62, 71,  2, 72, 63, 63, 62, 71, 61, 62],\n",
      "       [76,  2, 77, 58, 69, 68, 66, 71, 64,  2],\n",
      "       [58, 70, 62,  2, 60, 66, 75, 60, 69, 62]], dtype=int32))\n",
      "('\\ny\\n', array([[65, 58, 73, 77, 62, 75,  2, 17,  1,  0],\n",
      "       [76, 69, 62, 62, 79, 62,  2, 72, 63,  2],\n",
      "       [75, 14,  4,  1,  0,  1,  0, 48, 77, 62],\n",
      "       [ 2, 70, 62, 58, 71, 66, 71, 64,  1,  0],\n",
      "       [62, 75, 12,  2, 35, 66, 71, 72, 64, 62],\n",
      "       [ 1,  0, 76, 72,  2, 65, 62,  2, 65, 58],\n",
      "       [60, 66, 71, 64,  2, 65, 62, 75,  2, 65],\n",
      "       [71,  2, 72, 63, 63, 62, 71, 61, 62, 61],\n",
      "       [ 2, 77, 58, 69, 68, 66, 71, 64,  2, 58],\n",
      "       [70, 62,  2, 60, 66, 75, 60, 69, 62,  2]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hyperparam\n",
    "hidden_size = 100\n",
    "learning_rate = 1e-1\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"inputs\")\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"targets\")\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initiate_embedding_lookup(inputs, output_size, embedding_size):\n",
    "    embedding_weight = tf.get_variable('Embedding', [output_size, embedding_size])\n",
    "    embedding_lookup = tf.nn.embedding_lookup(embedding_weight, inputs)\n",
    "    return embedding_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vanilla_rnn(inputs, hidden_size, embedding_lookup):\n",
    "    inputs = tf.unstack(embedding_lookup, axis=1)\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(hidden_size)\n",
    "    output, state = tf.nn.static_rnn(cell, inputs, dtype=tf.float32)\n",
    "    return tf.reshape(output, shape=(-1, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(hidden_size, output, output_size):\n",
    "    weight = tf.get_variable('weight', [hidden_size, output_size])\n",
    "    bias = tf.get_variable('bias', [output_size])\n",
    "    logits = tf.matmul(output, weight) + bias\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(targets, output_size, logits):\n",
    "    one_hot_target = tf.one_hot(targets, output_size)\n",
    "    one_hot_target = tf.reshape(one_hot_target, shape=(-1, output_size))\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_target)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(learning_rate, loss):\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, targets = build_inputs(None, x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_lookup = initiate_embedding_lookup(inputs, len(vocab ), embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = build_vanilla_rnn(inputs, hidden_size, embedding_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = build_output(hidden_size, output, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = build_loss(targets, len(vocab), logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = build_optimizer(learning_rate, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(10):\n",
    "        for _x, _y in get_batches(encoded, 5, 50):\n",
    "            feed = {\n",
    "                inputs: _x,\n",
    "                targets: _y,\n",
    "            }\n",
    "            batch_loss, _ = sess.run([loss, opt], feed_dict=feed)\n",
    "            print batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
