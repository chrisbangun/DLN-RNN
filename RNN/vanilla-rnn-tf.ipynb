{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN on Tensorflow\n",
    "\n",
    "In this notebook, we'll learn how to build a character-wise rnn trained on Anna Karenina books.\n",
    "This tutorial is based on Andrej Karpathy's [RNN Post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning/raw/d94980095d1187998e2e0544966bb417f031390f/intro-to-rnns/assets/charseq.jpeg\" alt=\"Drawing\" style=\"width: 600px;\">\n",
    "\n",
    "The purpose of this tutorial is to learn step by step, starting from understanding the problem itself and the implementation of RNN.\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "As you can see from Andrej Karpathy's blog, RNN can be used to generate sequence of characters that later will start to learn of what is the most probable next characters given the current character as an input. In this tutorial we will do the same but with different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding the network of our input data, we need to do some sort of transformation first since our network doesn't understand characters. In usual machine learning problem, we feed the algorithm with a vector of features. This vector will be used by the algorithm to make a boundary decision.\n",
    "The situation is the same for RNN. Except we need to transform our input from sequence of characters into sequence of vectors. The most common approach for this transformation is by using one-hot encoding, pre-trained word embedding (you can have a look at gensim), or randomly generated vectors that later will be updated towards our training process. In this tutorial we will use the later approach by creating randomly generated vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first step, we'll load the text file and convert it into list of ids. The list of IDs will be used as an index for our lookup table. Lookup table? What is that? Don't get confuse, we'll get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# build our vocab by only use a set of characters that appear in our training data\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "# convert our list of characters to list of integers\n",
    "# for example: {'a': 1, 'b': 2}\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "\n",
    "# create a reverse lookup table to convert back the list of ids into characters\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "\n",
    "# create new dataset with all the text already converted into list of indices\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek into our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our real data:\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every unhappy family is unhappy in its own\n",
      "way.\n",
      "\n",
      "Everythin\n",
      "\n",
      "Our encoded data:\n",
      "[31 64 57 72 76 61 74  1 16  0  0  0 36 57 72 72 81  1 62 57 69 65 68 65 61\n",
      " 75  1 57 74 61  1 57 68 68  1 57 68 65 67 61 26  1 61 78 61 74 81  1 77 70\n",
      " 64 57 72 72 81  1 62 57 69 65 68 81  1 65 75  1 77 70 64 57 72 72 81  1 65\n",
      " 70  1 65 76 75  1 71 79 70  0 79 57 81 13  0  0 33 78 61 74 81 76 64 65 70]\n",
      "1985223\n"
     ]
    }
   ],
   "source": [
    "print(\"Our real data:\")\n",
    "print(text[:100])\n",
    "print(\"\\nOur encoded data:\")\n",
    "print(encoded[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have successfully transformed our data. Now in order to train our network, the next step is to split our data into several batch. So instead of putting all of our training data at once into our network, which could lead to vanishing/exploding gradient problem, longer time to converge, and possibly OOM error if you train this on GPU.\n",
    "\n",
    "Let's create a function that automatically generate the batch for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [ 1 57 69  1 70 71 76  1 63 71]\n",
      " [78 65 70 13  0  0  3 53 61 75]\n",
      " [70  1 60 77 74 65 70 63  1 64]\n",
      " [ 1 65 76  1 65 75 11  1 75 65]\n",
      " [ 1 37 76  1 79 57 75  0 71 70]\n",
      " [64 61 70  1 59 71 69 61  1 62]\n",
      " [26  1 58 77 76  1 70 71 79  1]\n",
      " [76  1 65 75 70  7 76 13  1 48]\n",
      " [ 1 75 57 65 60  1 76 71  1 64]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [57 69  1 70 71 76  1 63 71 65]\n",
      " [65 70 13  0  0  3 53 61 75 11]\n",
      " [ 1 60 77 74 65 70 63  1 64 65]\n",
      " [65 76  1 65 75 11  1 75 65 74]\n",
      " [37 76  1 79 57 75  0 71 70 68]\n",
      " [61 70  1 59 71 69 61  1 62 71]\n",
      " [ 1 58 77 76  1 70 71 79  1 75]\n",
      " [ 1 65 75 70  7 76 13  1 48 64]\n",
      " [75 57 65 60  1 76 71  1 64 61]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `get_batches` function has done a great job for us to generate our batch. Okay now we have done all the preparations and let's go to the fun stuff. Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RNN, as we may know, we need to employ the RNN-cell multiple times (called steps), in this case from the beginning of our sentence and usually until the end of line. For the purpose of our learning, we can start small by splitting each of the sentences into 5 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparam\n",
    "num_steps = 5\n",
    "bach_size = 50\n",
    "hidden_size = 100\n",
    "learning_rate = 1e-1\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    \"\"\"Placeholders in tensorflow act as a container of our input and later will\n",
    "    be fed into the tensorflow engine\n",
    "    \"\"\"\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"inputs\")\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"targets\")\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we convert all of our input into a sequence of ids?\n",
    "This is where all get interesting, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initiate_embedding_lookup(inputs, output_size, embedding_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    embedding_weight = tf.get_variable('Embedding', [output_size, embedding_size])\n",
    "    embedding_lookup = tf.nn.embedding_lookup(embedding_weight, inputs)\n",
    "    return embedding_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vanilla_rnn(inputs, hidden_size, embedding_lookup):\n",
    "    inputs = tf.unstack(embedding_lookup, axis=1)\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(hidden_size)\n",
    "    output, state = tf.nn.static_rnn(cell, inputs, dtype=tf.float32)\n",
    "    return tf.reshape(output, shape=(-1, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(hidden_size, output, output_size):\n",
    "    weight = tf.get_variable('weight', [hidden_size, output_size])\n",
    "    bias = tf.get_variable('bias', [output_size])\n",
    "    logits = tf.matmul(output, weight) + bias\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(targets, output_size, logits):\n",
    "    one_hot_target = tf.one_hot(targets, output_size)\n",
    "    one_hot_target = tf.reshape(one_hot_target, shape=(-1, output_size))\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_target)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(learning_rate, loss):\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, targets = build_inputs(None, x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_lookup = initiate_embedding_lookup(inputs, len(vocab ), embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = build_vanilla_rnn(inputs, hidden_size, embedding_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = build_output(hidden_size, output, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = build_loss(targets, len(vocab), logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = build_optimizer(learning_rate, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(10):\n",
    "        for _x, _y in get_batches(encoded, 5, 50):\n",
    "            feed = {\n",
    "                inputs: _x,\n",
    "                targets: _y,\n",
    "            }\n",
    "            batch_loss, _ = sess.run([loss, opt], feed_dict=feed)\n",
    "            print batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
